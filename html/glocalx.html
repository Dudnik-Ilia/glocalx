<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>glocalx API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>glocalx</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from abc import abstractmethod
from collections import defaultdict
from copy import deepcopy
from functools import reduce
from itertools import product
import os

import pickle

# Future warning silencing for train_test_split future warning
import warnings
warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning)


from logzero import logger

from numpy import argmax, logical_and, mean, std, percentile, array
from sklearn.model_selection import train_test_split
import tensorflow as tf

from evaluators import MemEvaluator
from callbacks import final_rule_dump_cb as final_rule_dump_callback
from models import Rule


class Predictor:
    &#34;&#34;&#34;Interface to be implemented by black boxes.&#34;&#34;&#34;
    @abstractmethod
    def predict(self, x):
        &#34;&#34;&#34;
        Predict instance(s) `x`

        Args:
            x (ndarray): The instance(s) to predict
        Returns:
            numpy.ndarray: Array of predictions
        &#34;&#34;&#34;
        pass


def shut_up_tensorflow():
    &#34;&#34;&#34;Silences tensorflow warnings.&#34;&#34;&#34;
    os.environ[&#34;KMP_AFFINITY&#34;] = &#34;noverbose&#34;
    os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39;
    tf.get_logger().setLevel(&#39;ERROR&#39;)
    tf.autograph.set_verbosity(3)


class GLocalX:
    &#34;&#34;&#34;
    GLocalX instance. Aggregates local explanations into global ones.

    Attributes:
        oracle (Predictor): The black box to explain
        intersecting (str): The explanation overlap strategy: either &#39;coverage&#39; or &#39;polyhedra&#39;
        high_concordance (bool): Use stringent join
        strong_cut (bool): Use stringent cut
        name (str): Name to use for log and output files
        evaluator (MemEvaluator): Evaluator used to evaluate merges and distances
        fine_boundary (set): Explanation boundary

    &#34;&#34;&#34;

    oracle: Predictor
    intersecting: str
    high_concordance: bool
    strong_cut: bool
    name: str
    evaluator: MemEvaluator
    fine_boundary: set

    def __init__(self, oracle=None, intersecting=&#39;coverage&#39;, name=None, high_concordance=True, strong_cut=False):
        self.oracle = oracle
        self.intersecting = intersecting
        self.high_concordance = high_concordance
        self.strong_cut = strong_cut
        self.name = name
        self.evaluator = MemEvaluator(oracle=self.oracle)

    @staticmethod
    def batch(y, sample_size=128):
        &#34;&#34;&#34;
        Sample `sample_size` objects from `x`.
        Args:
            y (ndarray): Labels.
            sample_size (int): Number of samples.
        Returns:
            numpy.ndarray: Indices of the sampled data.
        &#34;&#34;&#34;
        idx_train, *rest = train_test_split(range(y.size), shuffle=True, stratify=y, train_size=sample_size)

        return idx_train

    def partition(self, A, B, record=None):
        &#34;&#34;&#34;
        Find the conflicting, non-conflicting and disjoint groups between ruleset `A` and `B`.
        Args:
            A (list): List of rules.
            B (list): List of rules.
            record (int): Id of the record, if not None.
        Returns:
            tuple: Conflicting groups, non-conflicting groups, disjoint groups.
        &#34;&#34;&#34;
        conflicting_groups = list()
        non_conflicting_groups = list()
        disjoint_A, disjoint_B = {a for a in A}, {b for b in B}
        for i, a in enumerate(A):
            coverage_a = self.evaluator.coverages[a] if record is None\
                                                    else self.evaluator.coverages[a][record]
            conflicting_a = set()
            non_conflicting_a = set()

            for j, b in enumerate(B):
                coverage_b = self.evaluator.coverages[b] if record is None\
                                                        else self.evaluator.coverages[b][record]

                if (a, b) in self.evaluator.intersecting:
                    a_intersecting_b = self.evaluator.intersecting[(a, b)]
                elif (b, a) in self.evaluator.intersecting:
                    a_intersecting_b = self.evaluator.intersecting[(b, a)]
                elif not ((b, a) in self.evaluator.intersecting or (a, b) in self.evaluator.intersecting):
                    if self.intersecting == &#39;coverage&#39;:
                        a_intersecting_b = (logical_and(coverage_a, coverage_b)).any()
                    else:
                        a_intersecting_b = a &amp; b
                    self.evaluator.intersecting[(a, b)] = a_intersecting_b
                    self.evaluator.intersecting[(b, a)] = a_intersecting_b
                else:
                    a_intersecting_b = False

                if a_intersecting_b:
                    # Different consequence: conflicting
                    if a.consequence != b.consequence:
                        conflicting_a.add(a)
                        conflicting_a.add(b)
                        disjoint_A = disjoint_A - {a}
                        disjoint_B = disjoint_B - {b}

                    # Same consequence: non-conflicting
                    elif a.consequence == b.consequence:
                        non_conflicting_a.add(a)
                        non_conflicting_a.add(b)
                        disjoint_A = disjoint_A - {a}
                        disjoint_B = disjoint_B - {b}

            conflicting_groups.append(conflicting_a)
            non_conflicting_groups.append(non_conflicting_a)

        disjoint = disjoint_A | disjoint_B

        return conflicting_groups, non_conflicting_groups, disjoint

    def _cut(self, conflicting_group, x, y):
        &#34;&#34;&#34;
        Cut the provided `conflicting_groups`. Each conflicting group is
        a list of conflicting rules holding a &#39;king rule&#39; with dominance
        over the others. Cut is performed between the king rule and every other
        rule in the group. A non-king rule is cut each time is designed as such.
        Arguments:
            conflicting_group (iterable): Set of conflicting groups.
            x (ndarray): Data.
            y (ndarray): Labels.
        Returns:
            List: List of rules with minimized conflict.
        &#34;&#34;&#34;
        conflicting_group_list = list(conflicting_group)
        if len(conflicting_group_list) == 0:
            return conflicting_group

        cut_rules = set()
        default = int(y.mean().round())
        # Set ids to None to measure global fidelity_weight
        fidelities = array([self.evaluator.binary_fidelity(rule, x, y, default=default, ids=None)
                            for rule in conflicting_group_list])
        dominant_rule = conflicting_group_list[argmax(fidelities).item(0)]
        cut_rules.add(dominant_rule)

        for rule in conflicting_group - {dominant_rule}:
            dominant_features = dominant_rule.features
            cut_rule = rule - dominant_rule
            if self.strong_cut:
                for r in cut_rule:
                    for f in dominant_features - r.features:
                        if f in r.features:
                            r.features = r.features - {f}
                            del r[f]
            cut_rules |= cut_rule
        cut_rules.add(dominant_rule)

        return cut_rules

    def _join(self, rules, x, y):
        &#34;&#34;&#34;
        Join concordant rules.
        Arguments:
            rules (iterable): List of sets of conflicting groups.
            x (ndarray): Data.
            y (ndarray): Labels.
        Returns:
            set: List of rules with minimized conflict.
        &#34;&#34;&#34;
        # On an empty A_ set or B_ set, return the best rule of the non empty set.
        rules_list = list(rules)
        nr_rules = len(rules_list)
        if nr_rules == 0:
            return rules

        # List of ranges on each feature
        ranges_per_feature = defaultdict(list)
        for rule in rules_list:
            for feature, values in rule:
                ranges_per_feature[feature].append(values)

        default = int(y.mean().round())
        # ids set to None to measure global fidelity_weight
        fidelities = array([self.evaluator.binary_fidelity(r, x, y, default=default, ids=None) for r in rules_list])
        best_rule = rules_list[argmax(fidelities).item(0)]

        # Features shared by all
        shared_features = {f: ranges_per_feature[f] for f in ranges_per_feature
                           if len(ranges_per_feature[f]) == nr_rules}
        # Features not shared by all and from the best rule
        non_shared_features = {k: v for k, v in best_rule if k not in shared_features}

        premises = {}
        consequence = best_rule.consequence
        for f, values in shared_features.items():
            lower_bound, upper_bound = min([lb for lb, _ in values]), max([ub for _, ub in values])
            premises[f] = (lower_bound, upper_bound)
        # A highly-concordant merge includes non-shared features, hence making the join more stringent
        if not self.high_concordance:
            premises.update(non_shared_features)
        rule = Rule(premises=premises, consequence=consequence)

        return {rule}

    def merge(self, A, B, x, y, ids=None):
        &#34;&#34;&#34;
        Merge the two rulesets.
        Args:
            A (set): Set of rules.
            B (set): Set of rules.
            x (ndarray): Data.
            y (ndarray): Labels.
            ids (iterable): Ids of the records.
        Returns:
            set: Set of merged rules.
        &#34;&#34;&#34;
        AB = set()
        A_, B_ = list(A), list(B)

        # Compute the disjoint group and add it to AB
        _, _, disjoint_group = self.partition(A_, B_, ids)
        for record in ids:
            conflicting_group, non_conflicting_group, _ = self.partition(A_, B_, record)
            conflicting_group, non_conflicting_group = conflicting_group[0], non_conflicting_group[0]
            disjoint_group = disjoint_group - conflicting_group - non_conflicting_group

            cut_rules = self._cut(conflicting_group, x, y)
            joined_rules = self._join(non_conflicting_group, x, y)
            AB |= joined_rules
            AB |= cut_rules

        AB |= disjoint_group

        return AB

    # noinspection PyAttributeOutsideInit,PyAttributeOutsideInit
    def fit(self, rules, tr_set, batch_size=128, global_direction=False, callbacks=None,
            fidelity_weight=1., complexity_weight=1., callback_step=5, pickle_this=False):
        &#34;&#34;&#34;
        Train GLocalX on the given `rules`.
        Args:
            rules (list): List of rules.
            tr_set (ndarray): Training set (records).
            batch_size (int): Batch size. Defaults to 128.
            global_direction (bool): False to compute the BIC on the data batch,
                                    True to compute it on the whole validation set.
                                    Defaults to False.
            callbacks (list): List of callbacks to use. Defaults to the empty list.
            fidelity_weight (float): Weight to fidelity_weight (BIC-wise). Defaults to 1 (no weight).
            complexity_weight (float): Weight to complexity_weight (BIC-wise). Defaults to 1 (no weight).
            callback_step (Union(int, float)): Evoke the callbacks every `callback_step` iterations.
                                                Use float in [0, 1] to use percentage or an integer.
                                                Defaults to 5.
            pickle_this (bool): Whether to dump a pickle for this instance as the training finishes.
                                Defaults to False.
        Returns:
            GLocalX: Returns this trained instance.
        &#34;&#34;&#34;
        x, y = tr_set[:, :-1], tr_set[:, -1]
        if self.oracle is not None:
            y = self.oracle.predict(tr_set[:, :-1]).round().astype(int)
            tr_set[:, -1] = y.reshape(1, -1)

        m = len(rules)
        default = int(y.mean().round())
        input_rules = [{rule} for rule in rules]
        boundary = input_rules
        # The boundary vector holds the current currently available theories
        self.boundary = boundary
        self.boundary_len = len(self.boundary)
        self.fine_boundary = set(rules)

        # Merge
        iteration = 0
        merged = False
        rejections_list = list()
        while len(self.boundary) &gt; 2 and (merged or iteration == 0):
            logger.debug(self.name + &#39; | ************************ Iteration &#39; + str(iteration) + &#39; of &#39; + str(m))
            merged = False

            # Update distance matrix
            candidates_indices = [(i, j) for i, j in product(range(self.boundary_len), range(self.boundary_len))
                                  if j &gt; i]
            logger.debug(&#39;Computing distances&#39;)
            distances = [(i, j, self.evaluator.distance(self.boundary[i], self.boundary[j], x))
                         for i, j in candidates_indices]
            logger.debug(self.name + &#39;|  sorting candidates queue&#39;)
            candidates = sorted(distances, key=lambda c: c[2][0])
            # No available candidates, distances go from 0 to 1
            if len(candidates) == 0 or candidates[0][-1][0] == 1:
                break

            # Sample a data batch
            batch_ids = GLocalX.batch(y, sample_size=batch_size)
            # Explore candidates
            rejections = 0
            merge_difference = set()
            A, B, AB_union, AB_merge = None, None, None, None
            logger.debug(self.name + &#39; creating fine boundary&#39;)
            self.fine_boundary = set(reduce(lambda b, a: a.union(b), self.boundary, set()))
            for candidate_nr, (i, j, distance) in enumerate(candidates):
                A, B = self.boundary[i], self.boundary[j]
                AB_union = A | B
                AB_merge = self.merge(A, B, x, y, ids=batch_ids)
                logger.debug(self.name + &#39; merged candidate &#39; + str(rejections))
                merge_difference = deepcopy(AB_union - AB_merge)
                # Boundary without the potentially merging theories
                non_merging_boundary = [self.boundary[k] for k in range(self.boundary_len) if k != i and k != j]

                # BIC computation
                bic_union = self.evaluator.bic(AB_union, tr_set,
                                               fidelity_weight=fidelity_weight, complexity_weight=complexity_weight)
                bic_merge = self.evaluator.bic(AB_merge, tr_set,
                                               fidelity_weight=fidelity_weight, complexity_weight=complexity_weight)
                bic_union_validation, bic_merge_validation = bic_union, bic_merge
                if global_direction:
                    union_boundary = set(reduce(lambda b, a: a.union(b), [AB_union] + non_merging_boundary, set()))
                    merge_boundary = set(reduce(lambda b, a: a.union(b), [AB_merge] + non_merging_boundary, set()))

                    bic_union_global = self.evaluator.bic(union_boundary, tr_set)
                    bic_merge_global = self.evaluator.bic(merge_boundary, tr_set)

                    bic_union_validation, bic_merge_validation = bic_union_global, bic_merge_global

                if bic_merge_validation &lt;= bic_union_validation:
                    merged = True
                    rejections_list.append(rejections)
                    logger.debug(self.name + &#39; Merged candidate &#39; + str(rejections))
                    # Boundary update: the merging theories are removed and the merged theory is inserted
                    self.boundary = [AB_merge] + non_merging_boundary
                    self.boundary_len -= 1
                    self.fine_boundary = set(reduce(lambda b, a: a.union(b), self.boundary, set()))

                    break
                else:
                    rejections += 1

            # Callbacks
            if (iteration + 1) % callback_step == 0 and merged and callbacks is not None:
                logger.debug(self.name + &#39; Callbacks... &#39;)
                nr_rules_union, nr_rules_merge = len(AB_union), len(AB_merge)
                coverage_union = self.evaluator.coverage(AB_union, x, ids=batch_ids)
                coverage_merge = self.evaluator.coverage(AB_merge, x, ids=batch_ids)
                union_mean_rules_len = mean([len(r) for r in AB_union])
                union_std_rules_len = std([len(r) for r in AB_union])
                merge_mean_rules_len = mean([len(r) for r in AB_merge])
                merge_std_rules_len = std([len(r) for r in AB_merge])
                self.fine_boundary = set(reduce(lambda b, a: a.union(b), self.boundary, set()))
                fine_boundary_size = len(self.fine_boundary)

                for callback in callbacks:
                    # noinspection PyUnboundLocalVariable
                    callback(self, iteration=iteration, x=x, y=y, default=default,
                             callbacks_step=callback_step,
                             bic_union=bic_union, bic_merge=bic_merge, winner=(i, j),
                             nr_rules_union=nr_rules_union, nr_rules_merge=nr_rules_merge,
                             coverage_union=coverage_union, coverage_merge=coverage_merge,
                             fine_boundary=self.fine_boundary, m=m,
                             union_mean_rules_len=union_mean_rules_len, merge_mean_rules_len=merge_mean_rules_len,
                             union_std_rules_len=union_std_rules_len, merge_std_rules_len=merge_std_rules_len,
                             fine_boundary_size=fine_boundary_size, merged=merged,
                             rejections=rejections)

            # Iteration update
            iteration += 1

            # Thin out the evaluator by removing the references to non-existing rules and theories
            logger.debug(self.name + &#39; Forgetting &#39;)
            self.evaluator = self.evaluator.forget(merge_difference, A=A, B=B) if merged\
                             else self.evaluator.forget(merge_difference)

        # Final rule dump
        logger.debug(self.name + &#39; Dumping &#39;)
        final_rule_dump_callback(self, merged=False)

        # Pickle this instance
        if pickle_this:
            with open(self.name + &#39;.glocalx.pickle&#39;, &#39;wb&#39;) as log:
                pickle.dump(self, log)

        return self

    def rules(self, alpha=0.5, data=None, evaluator=None, is_percentile=False):
        &#34;&#34;&#34;
        Return the fine boundary of this instance, filtered by `alpha`.
        Args:
            alpha (Union(float | int)): Pruning factor, set to None for no pruning. Defaults to 0.5.
                                        For fidelity pruning use a float in [0, 1]. For percentile
                                        pruning use a float in [0, 1] and set `percentile` to True.
                                        For a number of rules, use a positive int.
            data (ndarray): Data (labels included).
            evaluator (Evaluator): Evaluator to use to prune, if any. None otherwise. Defaults to None.
            is_percentile (bool): Whether alpha is a percentile or a fidelity value.
        Returns:
            list: Fine boundary after a fit.
        &#34;&#34;&#34;
        if evaluator is None:
            evaluator_ = self.evaluator
        else:
            evaluator_ = evaluator

        fine_boundary = self.fine_boundary

        if data is None:
            return fine_boundary
        elif alpha is not None and len(fine_boundary) &gt; 0:
            x, y = data[:, :-1], data[:, -1]
            default = int(y.mean().round())
            rules_0 = [r for r in fine_boundary if r.consequence == 0]
            rules_1 = [r for r in fine_boundary if r.consequence == 1]
            fidelities_0 = [evaluator_.binary_fidelity(rule, x, y, default=default) for rule in rules_0]
            fidelities_1 = [evaluator_.binary_fidelity(rule, x, y, default=default) for rule in rules_1]
            if isinstance(alpha, float):
                lower_bound_0 = percentile(list(set(fidelities_0)), alpha) if is_percentile else alpha
                lower_bound_1 = percentile(list(set(fidelities_1)), alpha) if is_percentile else alpha

                fine_boundary_0 = [rule for rule, fidelity in zip(rules_0, fidelities_0) if fidelity &gt;= lower_bound_0]
                fine_boundary_1 = [rule for rule, fidelity in zip(rules_1, fidelities_1) if fidelity &gt;= lower_bound_1]
            else:
                fine_boundary_0 = sorted(zip(rules_0, fidelities_0), key=lambda el: el[1])[-alpha // 2:]
                fine_boundary_1 = sorted(zip(rules_0, fidelities_1), key=lambda el: el[1])[-alpha // 2:]

            return fine_boundary_0 + fine_boundary_1

        return None</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="glocalx.shut_up_tensorflow"><code class="name flex">
<span>def <span class="ident">shut_up_tensorflow</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Silences tensorflow warnings.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shut_up_tensorflow():
    &#34;&#34;&#34;Silences tensorflow warnings.&#34;&#34;&#34;
    os.environ[&#34;KMP_AFFINITY&#34;] = &#34;noverbose&#34;
    os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39;
    tf.get_logger().setLevel(&#39;ERROR&#39;)
    tf.autograph.set_verbosity(3)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="glocalx.GLocalX"><code class="flex name class">
<span>class <span class="ident">GLocalX</span></span>
<span>(</span><span>oracle=None, intersecting='coverage', name=None, high_concordance=True, strong_cut=False)</span>
</code></dt>
<dd>
<div class="desc"><p>GLocalX instance. Aggregates local explanations into global ones.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>oracle</code></strong> :&ensp;<code><a title="glocalx.Predictor" href="#glocalx.Predictor">Predictor</a></code></dt>
<dd>The black box to explain</dd>
<dt><strong><code>intersecting</code></strong> :&ensp;<code>str</code></dt>
<dd>The explanation overlap strategy: either 'coverage' or 'polyhedra'</dd>
<dt><strong><code>high_concordance</code></strong> :&ensp;<code>bool</code></dt>
<dd>Use stringent join</dd>
<dt><strong><code>strong_cut</code></strong> :&ensp;<code>bool</code></dt>
<dd>Use stringent cut</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name to use for log and output files</dd>
<dt><strong><code>evaluator</code></strong> :&ensp;<code>MemEvaluator</code></dt>
<dd>Evaluator used to evaluate merges and distances</dd>
<dt><strong><code>fine_boundary</code></strong> :&ensp;<code>set</code></dt>
<dd>Explanation boundary</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GLocalX:
    &#34;&#34;&#34;
    GLocalX instance. Aggregates local explanations into global ones.

    Attributes:
        oracle (Predictor): The black box to explain
        intersecting (str): The explanation overlap strategy: either &#39;coverage&#39; or &#39;polyhedra&#39;
        high_concordance (bool): Use stringent join
        strong_cut (bool): Use stringent cut
        name (str): Name to use for log and output files
        evaluator (MemEvaluator): Evaluator used to evaluate merges and distances
        fine_boundary (set): Explanation boundary

    &#34;&#34;&#34;

    oracle: Predictor
    intersecting: str
    high_concordance: bool
    strong_cut: bool
    name: str
    evaluator: MemEvaluator
    fine_boundary: set

    def __init__(self, oracle=None, intersecting=&#39;coverage&#39;, name=None, high_concordance=True, strong_cut=False):
        self.oracle = oracle
        self.intersecting = intersecting
        self.high_concordance = high_concordance
        self.strong_cut = strong_cut
        self.name = name
        self.evaluator = MemEvaluator(oracle=self.oracle)

    @staticmethod
    def batch(y, sample_size=128):
        &#34;&#34;&#34;
        Sample `sample_size` objects from `x`.
        Args:
            y (ndarray): Labels.
            sample_size (int): Number of samples.
        Returns:
            numpy.ndarray: Indices of the sampled data.
        &#34;&#34;&#34;
        idx_train, *rest = train_test_split(range(y.size), shuffle=True, stratify=y, train_size=sample_size)

        return idx_train

    def partition(self, A, B, record=None):
        &#34;&#34;&#34;
        Find the conflicting, non-conflicting and disjoint groups between ruleset `A` and `B`.
        Args:
            A (list): List of rules.
            B (list): List of rules.
            record (int): Id of the record, if not None.
        Returns:
            tuple: Conflicting groups, non-conflicting groups, disjoint groups.
        &#34;&#34;&#34;
        conflicting_groups = list()
        non_conflicting_groups = list()
        disjoint_A, disjoint_B = {a for a in A}, {b for b in B}
        for i, a in enumerate(A):
            coverage_a = self.evaluator.coverages[a] if record is None\
                                                    else self.evaluator.coverages[a][record]
            conflicting_a = set()
            non_conflicting_a = set()

            for j, b in enumerate(B):
                coverage_b = self.evaluator.coverages[b] if record is None\
                                                        else self.evaluator.coverages[b][record]

                if (a, b) in self.evaluator.intersecting:
                    a_intersecting_b = self.evaluator.intersecting[(a, b)]
                elif (b, a) in self.evaluator.intersecting:
                    a_intersecting_b = self.evaluator.intersecting[(b, a)]
                elif not ((b, a) in self.evaluator.intersecting or (a, b) in self.evaluator.intersecting):
                    if self.intersecting == &#39;coverage&#39;:
                        a_intersecting_b = (logical_and(coverage_a, coverage_b)).any()
                    else:
                        a_intersecting_b = a &amp; b
                    self.evaluator.intersecting[(a, b)] = a_intersecting_b
                    self.evaluator.intersecting[(b, a)] = a_intersecting_b
                else:
                    a_intersecting_b = False

                if a_intersecting_b:
                    # Different consequence: conflicting
                    if a.consequence != b.consequence:
                        conflicting_a.add(a)
                        conflicting_a.add(b)
                        disjoint_A = disjoint_A - {a}
                        disjoint_B = disjoint_B - {b}

                    # Same consequence: non-conflicting
                    elif a.consequence == b.consequence:
                        non_conflicting_a.add(a)
                        non_conflicting_a.add(b)
                        disjoint_A = disjoint_A - {a}
                        disjoint_B = disjoint_B - {b}

            conflicting_groups.append(conflicting_a)
            non_conflicting_groups.append(non_conflicting_a)

        disjoint = disjoint_A | disjoint_B

        return conflicting_groups, non_conflicting_groups, disjoint

    def _cut(self, conflicting_group, x, y):
        &#34;&#34;&#34;
        Cut the provided `conflicting_groups`. Each conflicting group is
        a list of conflicting rules holding a &#39;king rule&#39; with dominance
        over the others. Cut is performed between the king rule and every other
        rule in the group. A non-king rule is cut each time is designed as such.
        Arguments:
            conflicting_group (iterable): Set of conflicting groups.
            x (ndarray): Data.
            y (ndarray): Labels.
        Returns:
            List: List of rules with minimized conflict.
        &#34;&#34;&#34;
        conflicting_group_list = list(conflicting_group)
        if len(conflicting_group_list) == 0:
            return conflicting_group

        cut_rules = set()
        default = int(y.mean().round())
        # Set ids to None to measure global fidelity_weight
        fidelities = array([self.evaluator.binary_fidelity(rule, x, y, default=default, ids=None)
                            for rule in conflicting_group_list])
        dominant_rule = conflicting_group_list[argmax(fidelities).item(0)]
        cut_rules.add(dominant_rule)

        for rule in conflicting_group - {dominant_rule}:
            dominant_features = dominant_rule.features
            cut_rule = rule - dominant_rule
            if self.strong_cut:
                for r in cut_rule:
                    for f in dominant_features - r.features:
                        if f in r.features:
                            r.features = r.features - {f}
                            del r[f]
            cut_rules |= cut_rule
        cut_rules.add(dominant_rule)

        return cut_rules

    def _join(self, rules, x, y):
        &#34;&#34;&#34;
        Join concordant rules.
        Arguments:
            rules (iterable): List of sets of conflicting groups.
            x (ndarray): Data.
            y (ndarray): Labels.
        Returns:
            set: List of rules with minimized conflict.
        &#34;&#34;&#34;
        # On an empty A_ set or B_ set, return the best rule of the non empty set.
        rules_list = list(rules)
        nr_rules = len(rules_list)
        if nr_rules == 0:
            return rules

        # List of ranges on each feature
        ranges_per_feature = defaultdict(list)
        for rule in rules_list:
            for feature, values in rule:
                ranges_per_feature[feature].append(values)

        default = int(y.mean().round())
        # ids set to None to measure global fidelity_weight
        fidelities = array([self.evaluator.binary_fidelity(r, x, y, default=default, ids=None) for r in rules_list])
        best_rule = rules_list[argmax(fidelities).item(0)]

        # Features shared by all
        shared_features = {f: ranges_per_feature[f] for f in ranges_per_feature
                           if len(ranges_per_feature[f]) == nr_rules}
        # Features not shared by all and from the best rule
        non_shared_features = {k: v for k, v in best_rule if k not in shared_features}

        premises = {}
        consequence = best_rule.consequence
        for f, values in shared_features.items():
            lower_bound, upper_bound = min([lb for lb, _ in values]), max([ub for _, ub in values])
            premises[f] = (lower_bound, upper_bound)
        # A highly-concordant merge includes non-shared features, hence making the join more stringent
        if not self.high_concordance:
            premises.update(non_shared_features)
        rule = Rule(premises=premises, consequence=consequence)

        return {rule}

    def merge(self, A, B, x, y, ids=None):
        &#34;&#34;&#34;
        Merge the two rulesets.
        Args:
            A (set): Set of rules.
            B (set): Set of rules.
            x (ndarray): Data.
            y (ndarray): Labels.
            ids (iterable): Ids of the records.
        Returns:
            set: Set of merged rules.
        &#34;&#34;&#34;
        AB = set()
        A_, B_ = list(A), list(B)

        # Compute the disjoint group and add it to AB
        _, _, disjoint_group = self.partition(A_, B_, ids)
        for record in ids:
            conflicting_group, non_conflicting_group, _ = self.partition(A_, B_, record)
            conflicting_group, non_conflicting_group = conflicting_group[0], non_conflicting_group[0]
            disjoint_group = disjoint_group - conflicting_group - non_conflicting_group

            cut_rules = self._cut(conflicting_group, x, y)
            joined_rules = self._join(non_conflicting_group, x, y)
            AB |= joined_rules
            AB |= cut_rules

        AB |= disjoint_group

        return AB

    # noinspection PyAttributeOutsideInit,PyAttributeOutsideInit
    def fit(self, rules, tr_set, batch_size=128, global_direction=False, callbacks=None,
            fidelity_weight=1., complexity_weight=1., callback_step=5, pickle_this=False):
        &#34;&#34;&#34;
        Train GLocalX on the given `rules`.
        Args:
            rules (list): List of rules.
            tr_set (ndarray): Training set (records).
            batch_size (int): Batch size. Defaults to 128.
            global_direction (bool): False to compute the BIC on the data batch,
                                    True to compute it on the whole validation set.
                                    Defaults to False.
            callbacks (list): List of callbacks to use. Defaults to the empty list.
            fidelity_weight (float): Weight to fidelity_weight (BIC-wise). Defaults to 1 (no weight).
            complexity_weight (float): Weight to complexity_weight (BIC-wise). Defaults to 1 (no weight).
            callback_step (Union(int, float)): Evoke the callbacks every `callback_step` iterations.
                                                Use float in [0, 1] to use percentage or an integer.
                                                Defaults to 5.
            pickle_this (bool): Whether to dump a pickle for this instance as the training finishes.
                                Defaults to False.
        Returns:
            GLocalX: Returns this trained instance.
        &#34;&#34;&#34;
        x, y = tr_set[:, :-1], tr_set[:, -1]
        if self.oracle is not None:
            y = self.oracle.predict(tr_set[:, :-1]).round().astype(int)
            tr_set[:, -1] = y.reshape(1, -1)

        m = len(rules)
        default = int(y.mean().round())
        input_rules = [{rule} for rule in rules]
        boundary = input_rules
        # The boundary vector holds the current currently available theories
        self.boundary = boundary
        self.boundary_len = len(self.boundary)
        self.fine_boundary = set(rules)

        # Merge
        iteration = 0
        merged = False
        rejections_list = list()
        while len(self.boundary) &gt; 2 and (merged or iteration == 0):
            logger.debug(self.name + &#39; | ************************ Iteration &#39; + str(iteration) + &#39; of &#39; + str(m))
            merged = False

            # Update distance matrix
            candidates_indices = [(i, j) for i, j in product(range(self.boundary_len), range(self.boundary_len))
                                  if j &gt; i]
            logger.debug(&#39;Computing distances&#39;)
            distances = [(i, j, self.evaluator.distance(self.boundary[i], self.boundary[j], x))
                         for i, j in candidates_indices]
            logger.debug(self.name + &#39;|  sorting candidates queue&#39;)
            candidates = sorted(distances, key=lambda c: c[2][0])
            # No available candidates, distances go from 0 to 1
            if len(candidates) == 0 or candidates[0][-1][0] == 1:
                break

            # Sample a data batch
            batch_ids = GLocalX.batch(y, sample_size=batch_size)
            # Explore candidates
            rejections = 0
            merge_difference = set()
            A, B, AB_union, AB_merge = None, None, None, None
            logger.debug(self.name + &#39; creating fine boundary&#39;)
            self.fine_boundary = set(reduce(lambda b, a: a.union(b), self.boundary, set()))
            for candidate_nr, (i, j, distance) in enumerate(candidates):
                A, B = self.boundary[i], self.boundary[j]
                AB_union = A | B
                AB_merge = self.merge(A, B, x, y, ids=batch_ids)
                logger.debug(self.name + &#39; merged candidate &#39; + str(rejections))
                merge_difference = deepcopy(AB_union - AB_merge)
                # Boundary without the potentially merging theories
                non_merging_boundary = [self.boundary[k] for k in range(self.boundary_len) if k != i and k != j]

                # BIC computation
                bic_union = self.evaluator.bic(AB_union, tr_set,
                                               fidelity_weight=fidelity_weight, complexity_weight=complexity_weight)
                bic_merge = self.evaluator.bic(AB_merge, tr_set,
                                               fidelity_weight=fidelity_weight, complexity_weight=complexity_weight)
                bic_union_validation, bic_merge_validation = bic_union, bic_merge
                if global_direction:
                    union_boundary = set(reduce(lambda b, a: a.union(b), [AB_union] + non_merging_boundary, set()))
                    merge_boundary = set(reduce(lambda b, a: a.union(b), [AB_merge] + non_merging_boundary, set()))

                    bic_union_global = self.evaluator.bic(union_boundary, tr_set)
                    bic_merge_global = self.evaluator.bic(merge_boundary, tr_set)

                    bic_union_validation, bic_merge_validation = bic_union_global, bic_merge_global

                if bic_merge_validation &lt;= bic_union_validation:
                    merged = True
                    rejections_list.append(rejections)
                    logger.debug(self.name + &#39; Merged candidate &#39; + str(rejections))
                    # Boundary update: the merging theories are removed and the merged theory is inserted
                    self.boundary = [AB_merge] + non_merging_boundary
                    self.boundary_len -= 1
                    self.fine_boundary = set(reduce(lambda b, a: a.union(b), self.boundary, set()))

                    break
                else:
                    rejections += 1

            # Callbacks
            if (iteration + 1) % callback_step == 0 and merged and callbacks is not None:
                logger.debug(self.name + &#39; Callbacks... &#39;)
                nr_rules_union, nr_rules_merge = len(AB_union), len(AB_merge)
                coverage_union = self.evaluator.coverage(AB_union, x, ids=batch_ids)
                coverage_merge = self.evaluator.coverage(AB_merge, x, ids=batch_ids)
                union_mean_rules_len = mean([len(r) for r in AB_union])
                union_std_rules_len = std([len(r) for r in AB_union])
                merge_mean_rules_len = mean([len(r) for r in AB_merge])
                merge_std_rules_len = std([len(r) for r in AB_merge])
                self.fine_boundary = set(reduce(lambda b, a: a.union(b), self.boundary, set()))
                fine_boundary_size = len(self.fine_boundary)

                for callback in callbacks:
                    # noinspection PyUnboundLocalVariable
                    callback(self, iteration=iteration, x=x, y=y, default=default,
                             callbacks_step=callback_step,
                             bic_union=bic_union, bic_merge=bic_merge, winner=(i, j),
                             nr_rules_union=nr_rules_union, nr_rules_merge=nr_rules_merge,
                             coverage_union=coverage_union, coverage_merge=coverage_merge,
                             fine_boundary=self.fine_boundary, m=m,
                             union_mean_rules_len=union_mean_rules_len, merge_mean_rules_len=merge_mean_rules_len,
                             union_std_rules_len=union_std_rules_len, merge_std_rules_len=merge_std_rules_len,
                             fine_boundary_size=fine_boundary_size, merged=merged,
                             rejections=rejections)

            # Iteration update
            iteration += 1

            # Thin out the evaluator by removing the references to non-existing rules and theories
            logger.debug(self.name + &#39; Forgetting &#39;)
            self.evaluator = self.evaluator.forget(merge_difference, A=A, B=B) if merged\
                             else self.evaluator.forget(merge_difference)

        # Final rule dump
        logger.debug(self.name + &#39; Dumping &#39;)
        final_rule_dump_callback(self, merged=False)

        # Pickle this instance
        if pickle_this:
            with open(self.name + &#39;.glocalx.pickle&#39;, &#39;wb&#39;) as log:
                pickle.dump(self, log)

        return self

    def rules(self, alpha=0.5, data=None, evaluator=None, is_percentile=False):
        &#34;&#34;&#34;
        Return the fine boundary of this instance, filtered by `alpha`.
        Args:
            alpha (Union(float | int)): Pruning factor, set to None for no pruning. Defaults to 0.5.
                                        For fidelity pruning use a float in [0, 1]. For percentile
                                        pruning use a float in [0, 1] and set `percentile` to True.
                                        For a number of rules, use a positive int.
            data (ndarray): Data (labels included).
            evaluator (Evaluator): Evaluator to use to prune, if any. None otherwise. Defaults to None.
            is_percentile (bool): Whether alpha is a percentile or a fidelity value.
        Returns:
            list: Fine boundary after a fit.
        &#34;&#34;&#34;
        if evaluator is None:
            evaluator_ = self.evaluator
        else:
            evaluator_ = evaluator

        fine_boundary = self.fine_boundary

        if data is None:
            return fine_boundary
        elif alpha is not None and len(fine_boundary) &gt; 0:
            x, y = data[:, :-1], data[:, -1]
            default = int(y.mean().round())
            rules_0 = [r for r in fine_boundary if r.consequence == 0]
            rules_1 = [r for r in fine_boundary if r.consequence == 1]
            fidelities_0 = [evaluator_.binary_fidelity(rule, x, y, default=default) for rule in rules_0]
            fidelities_1 = [evaluator_.binary_fidelity(rule, x, y, default=default) for rule in rules_1]
            if isinstance(alpha, float):
                lower_bound_0 = percentile(list(set(fidelities_0)), alpha) if is_percentile else alpha
                lower_bound_1 = percentile(list(set(fidelities_1)), alpha) if is_percentile else alpha

                fine_boundary_0 = [rule for rule, fidelity in zip(rules_0, fidelities_0) if fidelity &gt;= lower_bound_0]
                fine_boundary_1 = [rule for rule, fidelity in zip(rules_1, fidelities_1) if fidelity &gt;= lower_bound_1]
            else:
                fine_boundary_0 = sorted(zip(rules_0, fidelities_0), key=lambda el: el[1])[-alpha // 2:]
                fine_boundary_1 = sorted(zip(rules_0, fidelities_1), key=lambda el: el[1])[-alpha // 2:]

            return fine_boundary_0 + fine_boundary_1

        return None</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="glocalx.GLocalX.evaluator"><code class="name">var <span class="ident">evaluator</span> : evaluators.MemEvaluator</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="glocalx.GLocalX.fine_boundary"><code class="name">var <span class="ident">fine_boundary</span> : set</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="glocalx.GLocalX.high_concordance"><code class="name">var <span class="ident">high_concordance</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="glocalx.GLocalX.intersecting"><code class="name">var <span class="ident">intersecting</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="glocalx.GLocalX.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="glocalx.GLocalX.oracle"><code class="name">var <span class="ident">oracle</span> : <a title="glocalx.Predictor" href="#glocalx.Predictor">Predictor</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="glocalx.GLocalX.strong_cut"><code class="name">var <span class="ident">strong_cut</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="glocalx.GLocalX.batch"><code class="name flex">
<span>def <span class="ident">batch</span></span>(<span>y, sample_size=128)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample <code>sample_size</code> objects from <code>x</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Labels.</dd>
<dt><strong><code>sample_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Indices of the sampled data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def batch(y, sample_size=128):
    &#34;&#34;&#34;
    Sample `sample_size` objects from `x`.
    Args:
        y (ndarray): Labels.
        sample_size (int): Number of samples.
    Returns:
        numpy.ndarray: Indices of the sampled data.
    &#34;&#34;&#34;
    idx_train, *rest = train_test_split(range(y.size), shuffle=True, stratify=y, train_size=sample_size)

    return idx_train</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="glocalx.GLocalX.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, rules, tr_set, batch_size=128, global_direction=False, callbacks=None, fidelity_weight=1.0, complexity_weight=1.0, callback_step=5, pickle_this=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Train GLocalX on the given <code>rules</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rules</code></strong> :&ensp;<code>list</code></dt>
<dd>List of rules.</dd>
<dt><strong><code>tr_set</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Training set (records).</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size. Defaults to 128.</dd>
<dt><strong><code>global_direction</code></strong> :&ensp;<code>bool</code></dt>
<dd>False to compute the BIC on the data batch,
True to compute it on the whole validation set.
Defaults to False.</dd>
<dt><strong><code>callbacks</code></strong> :&ensp;<code>list</code></dt>
<dd>List of callbacks to use. Defaults to the empty list.</dd>
<dt><strong><code>fidelity_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Weight to fidelity_weight (BIC-wise). Defaults to 1 (no weight).</dd>
<dt><strong><code>complexity_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Weight to complexity_weight (BIC-wise). Defaults to 1 (no weight).</dd>
<dt>callback_step (Union(int, float)): Evoke the callbacks every <code>callback_step</code> iterations.</dt>
<dt>Use float in [0, 1] to use percentage or an integer.</dt>
<dt>Defaults to 5.</dt>
<dt><strong><code>pickle_this</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to dump a pickle for this instance as the training finishes.
Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="glocalx.GLocalX" href="#glocalx.GLocalX">GLocalX</a></code></dt>
<dd>Returns this trained instance.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, rules, tr_set, batch_size=128, global_direction=False, callbacks=None,
        fidelity_weight=1., complexity_weight=1., callback_step=5, pickle_this=False):
    &#34;&#34;&#34;
    Train GLocalX on the given `rules`.
    Args:
        rules (list): List of rules.
        tr_set (ndarray): Training set (records).
        batch_size (int): Batch size. Defaults to 128.
        global_direction (bool): False to compute the BIC on the data batch,
                                True to compute it on the whole validation set.
                                Defaults to False.
        callbacks (list): List of callbacks to use. Defaults to the empty list.
        fidelity_weight (float): Weight to fidelity_weight (BIC-wise). Defaults to 1 (no weight).
        complexity_weight (float): Weight to complexity_weight (BIC-wise). Defaults to 1 (no weight).
        callback_step (Union(int, float)): Evoke the callbacks every `callback_step` iterations.
                                            Use float in [0, 1] to use percentage or an integer.
                                            Defaults to 5.
        pickle_this (bool): Whether to dump a pickle for this instance as the training finishes.
                            Defaults to False.
    Returns:
        GLocalX: Returns this trained instance.
    &#34;&#34;&#34;
    x, y = tr_set[:, :-1], tr_set[:, -1]
    if self.oracle is not None:
        y = self.oracle.predict(tr_set[:, :-1]).round().astype(int)
        tr_set[:, -1] = y.reshape(1, -1)

    m = len(rules)
    default = int(y.mean().round())
    input_rules = [{rule} for rule in rules]
    boundary = input_rules
    # The boundary vector holds the current currently available theories
    self.boundary = boundary
    self.boundary_len = len(self.boundary)
    self.fine_boundary = set(rules)

    # Merge
    iteration = 0
    merged = False
    rejections_list = list()
    while len(self.boundary) &gt; 2 and (merged or iteration == 0):
        logger.debug(self.name + &#39; | ************************ Iteration &#39; + str(iteration) + &#39; of &#39; + str(m))
        merged = False

        # Update distance matrix
        candidates_indices = [(i, j) for i, j in product(range(self.boundary_len), range(self.boundary_len))
                              if j &gt; i]
        logger.debug(&#39;Computing distances&#39;)
        distances = [(i, j, self.evaluator.distance(self.boundary[i], self.boundary[j], x))
                     for i, j in candidates_indices]
        logger.debug(self.name + &#39;|  sorting candidates queue&#39;)
        candidates = sorted(distances, key=lambda c: c[2][0])
        # No available candidates, distances go from 0 to 1
        if len(candidates) == 0 or candidates[0][-1][0] == 1:
            break

        # Sample a data batch
        batch_ids = GLocalX.batch(y, sample_size=batch_size)
        # Explore candidates
        rejections = 0
        merge_difference = set()
        A, B, AB_union, AB_merge = None, None, None, None
        logger.debug(self.name + &#39; creating fine boundary&#39;)
        self.fine_boundary = set(reduce(lambda b, a: a.union(b), self.boundary, set()))
        for candidate_nr, (i, j, distance) in enumerate(candidates):
            A, B = self.boundary[i], self.boundary[j]
            AB_union = A | B
            AB_merge = self.merge(A, B, x, y, ids=batch_ids)
            logger.debug(self.name + &#39; merged candidate &#39; + str(rejections))
            merge_difference = deepcopy(AB_union - AB_merge)
            # Boundary without the potentially merging theories
            non_merging_boundary = [self.boundary[k] for k in range(self.boundary_len) if k != i and k != j]

            # BIC computation
            bic_union = self.evaluator.bic(AB_union, tr_set,
                                           fidelity_weight=fidelity_weight, complexity_weight=complexity_weight)
            bic_merge = self.evaluator.bic(AB_merge, tr_set,
                                           fidelity_weight=fidelity_weight, complexity_weight=complexity_weight)
            bic_union_validation, bic_merge_validation = bic_union, bic_merge
            if global_direction:
                union_boundary = set(reduce(lambda b, a: a.union(b), [AB_union] + non_merging_boundary, set()))
                merge_boundary = set(reduce(lambda b, a: a.union(b), [AB_merge] + non_merging_boundary, set()))

                bic_union_global = self.evaluator.bic(union_boundary, tr_set)
                bic_merge_global = self.evaluator.bic(merge_boundary, tr_set)

                bic_union_validation, bic_merge_validation = bic_union_global, bic_merge_global

            if bic_merge_validation &lt;= bic_union_validation:
                merged = True
                rejections_list.append(rejections)
                logger.debug(self.name + &#39; Merged candidate &#39; + str(rejections))
                # Boundary update: the merging theories are removed and the merged theory is inserted
                self.boundary = [AB_merge] + non_merging_boundary
                self.boundary_len -= 1
                self.fine_boundary = set(reduce(lambda b, a: a.union(b), self.boundary, set()))

                break
            else:
                rejections += 1

        # Callbacks
        if (iteration + 1) % callback_step == 0 and merged and callbacks is not None:
            logger.debug(self.name + &#39; Callbacks... &#39;)
            nr_rules_union, nr_rules_merge = len(AB_union), len(AB_merge)
            coverage_union = self.evaluator.coverage(AB_union, x, ids=batch_ids)
            coverage_merge = self.evaluator.coverage(AB_merge, x, ids=batch_ids)
            union_mean_rules_len = mean([len(r) for r in AB_union])
            union_std_rules_len = std([len(r) for r in AB_union])
            merge_mean_rules_len = mean([len(r) for r in AB_merge])
            merge_std_rules_len = std([len(r) for r in AB_merge])
            self.fine_boundary = set(reduce(lambda b, a: a.union(b), self.boundary, set()))
            fine_boundary_size = len(self.fine_boundary)

            for callback in callbacks:
                # noinspection PyUnboundLocalVariable
                callback(self, iteration=iteration, x=x, y=y, default=default,
                         callbacks_step=callback_step,
                         bic_union=bic_union, bic_merge=bic_merge, winner=(i, j),
                         nr_rules_union=nr_rules_union, nr_rules_merge=nr_rules_merge,
                         coverage_union=coverage_union, coverage_merge=coverage_merge,
                         fine_boundary=self.fine_boundary, m=m,
                         union_mean_rules_len=union_mean_rules_len, merge_mean_rules_len=merge_mean_rules_len,
                         union_std_rules_len=union_std_rules_len, merge_std_rules_len=merge_std_rules_len,
                         fine_boundary_size=fine_boundary_size, merged=merged,
                         rejections=rejections)

        # Iteration update
        iteration += 1

        # Thin out the evaluator by removing the references to non-existing rules and theories
        logger.debug(self.name + &#39; Forgetting &#39;)
        self.evaluator = self.evaluator.forget(merge_difference, A=A, B=B) if merged\
                         else self.evaluator.forget(merge_difference)

    # Final rule dump
    logger.debug(self.name + &#39; Dumping &#39;)
    final_rule_dump_callback(self, merged=False)

    # Pickle this instance
    if pickle_this:
        with open(self.name + &#39;.glocalx.pickle&#39;, &#39;wb&#39;) as log:
            pickle.dump(self, log)

    return self</code></pre>
</details>
</dd>
<dt id="glocalx.GLocalX.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self, A, B, x, y, ids=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Merge the two rulesets.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>A</code></strong> :&ensp;<code>set</code></dt>
<dd>Set of rules.</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>set</code></dt>
<dd>Set of rules.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Labels.</dd>
<dt><strong><code>ids</code></strong> :&ensp;<code>iterable</code></dt>
<dd>Ids of the records.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>set</code></dt>
<dd>Set of merged rules.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge(self, A, B, x, y, ids=None):
    &#34;&#34;&#34;
    Merge the two rulesets.
    Args:
        A (set): Set of rules.
        B (set): Set of rules.
        x (ndarray): Data.
        y (ndarray): Labels.
        ids (iterable): Ids of the records.
    Returns:
        set: Set of merged rules.
    &#34;&#34;&#34;
    AB = set()
    A_, B_ = list(A), list(B)

    # Compute the disjoint group and add it to AB
    _, _, disjoint_group = self.partition(A_, B_, ids)
    for record in ids:
        conflicting_group, non_conflicting_group, _ = self.partition(A_, B_, record)
        conflicting_group, non_conflicting_group = conflicting_group[0], non_conflicting_group[0]
        disjoint_group = disjoint_group - conflicting_group - non_conflicting_group

        cut_rules = self._cut(conflicting_group, x, y)
        joined_rules = self._join(non_conflicting_group, x, y)
        AB |= joined_rules
        AB |= cut_rules

    AB |= disjoint_group

    return AB</code></pre>
</details>
</dd>
<dt id="glocalx.GLocalX.partition"><code class="name flex">
<span>def <span class="ident">partition</span></span>(<span>self, A, B, record=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Find the conflicting, non-conflicting and disjoint groups between ruleset <code>A</code> and <code>B</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>A</code></strong> :&ensp;<code>list</code></dt>
<dd>List of rules.</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>list</code></dt>
<dd>List of rules.</dd>
<dt><strong><code>record</code></strong> :&ensp;<code>int</code></dt>
<dd>Id of the record, if not None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Conflicting groups, non-conflicting groups, disjoint groups.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def partition(self, A, B, record=None):
    &#34;&#34;&#34;
    Find the conflicting, non-conflicting and disjoint groups between ruleset `A` and `B`.
    Args:
        A (list): List of rules.
        B (list): List of rules.
        record (int): Id of the record, if not None.
    Returns:
        tuple: Conflicting groups, non-conflicting groups, disjoint groups.
    &#34;&#34;&#34;
    conflicting_groups = list()
    non_conflicting_groups = list()
    disjoint_A, disjoint_B = {a for a in A}, {b for b in B}
    for i, a in enumerate(A):
        coverage_a = self.evaluator.coverages[a] if record is None\
                                                else self.evaluator.coverages[a][record]
        conflicting_a = set()
        non_conflicting_a = set()

        for j, b in enumerate(B):
            coverage_b = self.evaluator.coverages[b] if record is None\
                                                    else self.evaluator.coverages[b][record]

            if (a, b) in self.evaluator.intersecting:
                a_intersecting_b = self.evaluator.intersecting[(a, b)]
            elif (b, a) in self.evaluator.intersecting:
                a_intersecting_b = self.evaluator.intersecting[(b, a)]
            elif not ((b, a) in self.evaluator.intersecting or (a, b) in self.evaluator.intersecting):
                if self.intersecting == &#39;coverage&#39;:
                    a_intersecting_b = (logical_and(coverage_a, coverage_b)).any()
                else:
                    a_intersecting_b = a &amp; b
                self.evaluator.intersecting[(a, b)] = a_intersecting_b
                self.evaluator.intersecting[(b, a)] = a_intersecting_b
            else:
                a_intersecting_b = False

            if a_intersecting_b:
                # Different consequence: conflicting
                if a.consequence != b.consequence:
                    conflicting_a.add(a)
                    conflicting_a.add(b)
                    disjoint_A = disjoint_A - {a}
                    disjoint_B = disjoint_B - {b}

                # Same consequence: non-conflicting
                elif a.consequence == b.consequence:
                    non_conflicting_a.add(a)
                    non_conflicting_a.add(b)
                    disjoint_A = disjoint_A - {a}
                    disjoint_B = disjoint_B - {b}

        conflicting_groups.append(conflicting_a)
        non_conflicting_groups.append(non_conflicting_a)

    disjoint = disjoint_A | disjoint_B

    return conflicting_groups, non_conflicting_groups, disjoint</code></pre>
</details>
</dd>
<dt id="glocalx.GLocalX.rules"><code class="name flex">
<span>def <span class="ident">rules</span></span>(<span>self, alpha=0.5, data=None, evaluator=None, is_percentile=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the fine boundary of this instance, filtered by <code>alpha</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt>alpha (Union(float | int)): Pruning factor, set to None for no pruning. Defaults to 0.5.</dt>
<dt>For fidelity pruning use a float in [0, 1]. For percentile</dt>
<dt>pruning use a float in [0, 1] and set <code>percentile</code> to True.</dt>
<dt>For a number of rules, use a positive int.</dt>
<dt><strong><code>data</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data (labels included).</dd>
<dt><strong><code>evaluator</code></strong> :&ensp;<code>Evaluator</code></dt>
<dd>Evaluator to use to prune, if any. None otherwise. Defaults to None.</dd>
<dt><strong><code>is_percentile</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether alpha is a percentile or a fidelity value.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>Fine boundary after a fit.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rules(self, alpha=0.5, data=None, evaluator=None, is_percentile=False):
    &#34;&#34;&#34;
    Return the fine boundary of this instance, filtered by `alpha`.
    Args:
        alpha (Union(float | int)): Pruning factor, set to None for no pruning. Defaults to 0.5.
                                    For fidelity pruning use a float in [0, 1]. For percentile
                                    pruning use a float in [0, 1] and set `percentile` to True.
                                    For a number of rules, use a positive int.
        data (ndarray): Data (labels included).
        evaluator (Evaluator): Evaluator to use to prune, if any. None otherwise. Defaults to None.
        is_percentile (bool): Whether alpha is a percentile or a fidelity value.
    Returns:
        list: Fine boundary after a fit.
    &#34;&#34;&#34;
    if evaluator is None:
        evaluator_ = self.evaluator
    else:
        evaluator_ = evaluator

    fine_boundary = self.fine_boundary

    if data is None:
        return fine_boundary
    elif alpha is not None and len(fine_boundary) &gt; 0:
        x, y = data[:, :-1], data[:, -1]
        default = int(y.mean().round())
        rules_0 = [r for r in fine_boundary if r.consequence == 0]
        rules_1 = [r for r in fine_boundary if r.consequence == 1]
        fidelities_0 = [evaluator_.binary_fidelity(rule, x, y, default=default) for rule in rules_0]
        fidelities_1 = [evaluator_.binary_fidelity(rule, x, y, default=default) for rule in rules_1]
        if isinstance(alpha, float):
            lower_bound_0 = percentile(list(set(fidelities_0)), alpha) if is_percentile else alpha
            lower_bound_1 = percentile(list(set(fidelities_1)), alpha) if is_percentile else alpha

            fine_boundary_0 = [rule for rule, fidelity in zip(rules_0, fidelities_0) if fidelity &gt;= lower_bound_0]
            fine_boundary_1 = [rule for rule, fidelity in zip(rules_1, fidelities_1) if fidelity &gt;= lower_bound_1]
        else:
            fine_boundary_0 = sorted(zip(rules_0, fidelities_0), key=lambda el: el[1])[-alpha // 2:]
            fine_boundary_1 = sorted(zip(rules_0, fidelities_1), key=lambda el: el[1])[-alpha // 2:]

        return fine_boundary_0 + fine_boundary_1

    return None</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="glocalx.Predictor"><code class="flex name class">
<span>class <span class="ident">Predictor</span></span>
</code></dt>
<dd>
<div class="desc"><p>Interface to be implemented by black boxes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Predictor:
    &#34;&#34;&#34;Interface to be implemented by black boxes.&#34;&#34;&#34;
    @abstractmethod
    def predict(self, x):
        &#34;&#34;&#34;
        Predict instance(s) `x`

        Args:
            x (ndarray): The instance(s) to predict
        Returns:
            numpy.ndarray: Array of predictions
        &#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="glocalx.Predictor.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict instance(s) <code>x</code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>The instance(s) to predict</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Array of predictions</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def predict(self, x):
    &#34;&#34;&#34;
    Predict instance(s) `x`

    Args:
        x (ndarray): The instance(s) to predict
    Returns:
        numpy.ndarray: Array of predictions
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="glocalx.shut_up_tensorflow" href="#glocalx.shut_up_tensorflow">shut_up_tensorflow</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="glocalx.GLocalX" href="#glocalx.GLocalX">GLocalX</a></code></h4>
<ul class="two-column">
<li><code><a title="glocalx.GLocalX.batch" href="#glocalx.GLocalX.batch">batch</a></code></li>
<li><code><a title="glocalx.GLocalX.evaluator" href="#glocalx.GLocalX.evaluator">evaluator</a></code></li>
<li><code><a title="glocalx.GLocalX.fine_boundary" href="#glocalx.GLocalX.fine_boundary">fine_boundary</a></code></li>
<li><code><a title="glocalx.GLocalX.fit" href="#glocalx.GLocalX.fit">fit</a></code></li>
<li><code><a title="glocalx.GLocalX.high_concordance" href="#glocalx.GLocalX.high_concordance">high_concordance</a></code></li>
<li><code><a title="glocalx.GLocalX.intersecting" href="#glocalx.GLocalX.intersecting">intersecting</a></code></li>
<li><code><a title="glocalx.GLocalX.merge" href="#glocalx.GLocalX.merge">merge</a></code></li>
<li><code><a title="glocalx.GLocalX.name" href="#glocalx.GLocalX.name">name</a></code></li>
<li><code><a title="glocalx.GLocalX.oracle" href="#glocalx.GLocalX.oracle">oracle</a></code></li>
<li><code><a title="glocalx.GLocalX.partition" href="#glocalx.GLocalX.partition">partition</a></code></li>
<li><code><a title="glocalx.GLocalX.rules" href="#glocalx.GLocalX.rules">rules</a></code></li>
<li><code><a title="glocalx.GLocalX.strong_cut" href="#glocalx.GLocalX.strong_cut">strong_cut</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="glocalx.Predictor" href="#glocalx.Predictor">Predictor</a></code></h4>
<ul class="">
<li><code><a title="glocalx.Predictor.predict" href="#glocalx.Predictor.predict">predict</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>